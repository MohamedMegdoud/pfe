{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f8adc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['riskAllele', 'pValue', 'pValueAnnotation', 'riskFrequency', 'orValue',\n",
      "       'beta', 'ci', 'mappedGenes', 'traitName', 'efoTraits', 'bgTraits',\n",
      "       'accessionId', 'locations', 'pubmedId', 'author'],\n",
      "      dtype='object')\n",
      "130    rs143332484_T\n",
      "131       rs616338_T\n",
      "133     rs75932628_T\n",
      "136    rs139967528_G\n",
      "183       rs593742_A\n",
      "Name: riskAllele, dtype: object\n",
      "SNPs with p-value < 1e-5:\n",
      "rs143332484_T\n",
      "rs616338_T\n",
      "rs75932628_T\n",
      "rs139967528_G\n",
      "rs593742_A\n",
      "rs7920721_G\n",
      "rs138190086_A\n",
      "rs114812713_C\n",
      "rs11168036_T\n",
      "rs283811_G\n",
      "rs283811_G\n",
      "rs483082_T\n",
      "rs6857_T\n",
      "rs11257238_T\n",
      "rs41289512_G\n",
      "rs4663105_C\n",
      "rs2632516_C\n",
      "rs9381563_C\n",
      "rs4236673_A\n",
      "rs11763230_T\n",
      "rs7935829_G\n",
      "rs1859788_A\n",
      "rs6656401_A\n",
      "rs6733839_T\n",
      "rs9331896_C\n",
      "rs983392_G\n",
      "rs10792832_A\n",
      "rs11218343_C\n",
      "rs4147929_A\n",
      "rs429358_C\n",
      "rs429358_C\n",
      "rs429358_C\n",
      "rs429358_C\n",
      "rs2075650_G\n",
      "rs11754661_A\n",
      "rs519113_G\n",
      "rs115550680_G\n",
      "rs6859_A\n",
      "rs2373115_G\n",
      "rs2718058_A\n",
      "rs11218343_T\n",
      "rs6656401_A\n",
      "rs6733839_T\n",
      "rs10948363_G\n",
      "rs11771145_G\n",
      "rs9331896_T\n",
      "rs983392_A\n",
      "rs10792832_G\n",
      "rs4147929_A\n",
      "rs1476679_T\n",
      "rs17125944_C\n",
      "rs9271192_C\n",
      "rs28834970_C\n",
      "rs10498633_G\n",
      "rs7561528_A\n",
      "rs6701713_A\n",
      "rs9349407_C\n",
      "rs2075650_G\n",
      "rs75932628_T\n",
      "rs679515_T\n",
      "rs6733839_T\n",
      "rs9331896_T\n",
      "rs4663105_C\n",
      "rs1582763_G\n",
      "rs2732703_T\n",
      "rs2075650_G\n",
      "rs394819_T\n",
      "rs920608_C\n",
      "rs12444183_A\n",
      "rs2154481_C\n",
      "rs4351014_C\n",
      "rs4985556_A\n",
      "rs72835061_A\n",
      "rs7342692_T\n",
      "rs79832570_C\n",
      "rs876461_A\n",
      "rs4663105_C\n",
      "rs429358_C\n",
      "rs117261169_T\n",
      "rs35255921_A\n",
      "rs35194383_T\n",
      "rs34874378_A\n",
      "rs112481437_A\n",
      "rs60239918_T\n",
      "rs78620885_T\n",
      "rs12461065_T\n",
      "rs28469095_C\n",
      "rs10469272_A\n",
      "rs595290_C\n",
      "rs73566293_A\n",
      "rs12461144_T\n",
      "rs6014724_G\n",
      "rs8111946_C\n",
      "rs151330717_A\n",
      "rs2965164_T\n",
      "rs11881756_C\n",
      "rs79638902_T\n",
      "rs2965169_C\n",
      "rs2967668_G\n",
      "rs28399637_A\n",
      "rs78986976_A\n",
      "rs147711004_A\n",
      "rs10407439_A\n",
      "rs55840414_A\n",
      "rs28615360_A\n",
      "rs150639620_T\n",
      "rs6859_A\n",
      "rs41290120_A\n",
      "rs157580_G\n",
      "rs1160983_A\n",
      "rs1160984_T\n",
      "rs141864196_A\n",
      "rs438811_T\n",
      "rs73033507_T\n",
      "rs204480_T\n",
      "rs2093760_A\n",
      "rs4663105_C\n",
      "rs1859788_A\n",
      "rs35251323_G\n",
      "rs4236673_A\n",
      "rs2081545_A\n",
      "rs867611_G\n",
      "rs11218343_C\n",
      "rs12590654_A\n",
      "rs28394864_A\n",
      "rs3795065_C\n",
      "rs80257887_A\n",
      "rs75161053_A\n",
      "rs11083742_G\n",
      "rs1160984_T\n",
      "rs7412_T\n",
      "rs1081105_C\n",
      "rs11672748_G\n",
      "rs35255921_A\n",
      "rs60239918_T\n",
      "rs78620885_T\n",
      "rs754366_A\n",
      "rs346758_G\n",
      "rs3818361_A\n",
      "rs4663105_C\n",
      "rs1859788_A\n",
      "rs1532276_T\n",
      "rs7114401_C\n",
      "rs203707_G\n",
      "rs714948_A\n",
      "rs2965169_C\n",
      "rs2967668_G\n",
      "rs28399637_A\n",
      "rs10407439_A\n",
      "rs406315_G\n",
      "rs79701229_A\n",
      "rs157580_G\n",
      "rs11556505_T\n",
      "rs2093760_A\n",
      "rs824523_A\n",
      "rs4663105_C\n",
      "rs145538381_C\n",
      "rs73069394_A\n",
      "rs77399788_G\n",
      "rs4236673_A\n",
      "rs10437655_A\n",
      "rs2081545_A\n",
      "rs867611_G\n",
      "rs11218343_C\n",
      "rs442495_C\n",
      "rs117618017_T\n",
      "rs28394864_A\n",
      "rs80257887_A\n",
      "rs846879_G\n",
      "rs8111946_C\n",
      "rs714948_A\n",
      "rs2965164_T\n",
      "rs4803750_G\n",
      "rs2965169_C\n",
      "rs2967668_G\n",
      "rs79701229_A\n",
      "rs7254892_A\n",
      "rs157580_G\n",
      "rs1160984_T\n",
      "rs1081105_C\n",
      "rs75627662_T\n",
      "rs34800911_G\n",
      "rs34874378_A\n",
      "rs76856627_G\n",
      "rs112481437_A\n",
      "rs28469095_C\n",
      "rs595290_C\n",
      "rs73566293_A\n",
      "rs2093760_A\n",
      "rs4663105_C\n",
      "rs34995835_T\n",
      "rs11787077_T\n",
      "rs10792832_A\n",
      "rs80257887_A\n",
      "rs846879_G\n",
      "rs10410651_C\n",
      "rs714948_A\n",
      "rs111740474_A\n",
      "rs2927437_A\n",
      "rs10401176_T\n",
      "rs2927477_C\n",
      "rs28399637_A\n",
      "rs147711004_A\n",
      "rs10407439_A\n",
      "rs55840414_A\n",
      "rs150639620_T\n",
      "rs6859_A\n",
      "rs41290120_A\n",
      "rs28399637_A\n",
      "rs147711004_A\n",
      "rs10407439_A\n",
      "rs55840414_A\n",
      "rs28615360_A\n",
      "rs365653_G\n",
      "rs6859_A\n",
      "rs157580_G\n",
      "rs1160983_A\n",
      "rs1160984_T\n",
      "rs138235833_G\n",
      "rs438811_T\n",
      "rs11672748_G\n",
      "rs34800911_G\n",
      "rs147188206_C\n",
      "rs60239918_T\n",
      "rs12461065_T\n",
      "rs28469095_C\n",
      "rs595290_C\n",
      "rs73566293_A\n",
      "rs429358_C\n",
      "rs440446_C\n",
      "rs4803764_C\n",
      "rs7412_T\n",
      "rs6859_A\n",
      "rs28399664_G\n",
      "rs6431220_A\n",
      "rs4748424_G\n",
      "rs429358_C\n",
      "rs405509_G\n",
      "rs7412_T\n",
      "rs6859_A\n",
      "rs28399637_A\n",
      "rs11668327_C\n",
      "rs60049679_C\n",
      "rs28399664_G\n",
      "rs2927468_A\n",
      "rs8103315_A\n",
      "rs2967668_G\n",
      "rs111371860_T\n",
      "rs138607350_G\n",
      "rs10952097_T\n",
      "rs679515_T\n",
      "rs6966331_T\n",
      "rs76928645_T\n",
      "rs143080277_C\n",
      "rs6733839_T\n",
      "rs34173062_A\n",
      "rs1800978_G\n",
      "rs7912495_G\n",
      "rs61762319_G\n",
      "rs3822030_G\n",
      "rs6584063_G\n",
      "rs7908662_G\n",
      "rs2245466_G\n",
      "rs112403360_A\n",
      "rs62374257_C\n",
      "rs871269_T\n",
      "rs113706587_A\n",
      "rs6605556_G\n",
      "rs10947943_A\n",
      "rs143332484_T\n",
      "rs75932628_T\n",
      "rs7767350_T\n",
      "rs785129_T\n",
      "rs12590654_A\n",
      "rs10131280_A\n",
      "rs602602_A\n",
      "rs117618017_T\n",
      "rs3848143_G\n",
      "rs12592898_A\n",
      "rs1140239_T\n",
      "rs889555_T\n",
      "rs4985556_A\n",
      "rs12446759_G\n",
      "rs72824905_G\n",
      "rs56407236_A\n",
      "rs35048651_T\n",
      "rs7225151_A\n",
      "rs2242595_A\n",
      "rs5848_T\n",
      "rs199515_G\n",
      "rs616338_T\n",
      "rs2526377_G\n",
      "rs4277405_C\n",
      "rs12151021_A\n",
      "rs141749679_C\n",
      "rs13237518_A\n",
      "rs17020490_C\n",
      "rs7384878_C\n",
      "rs11771145_A\n",
      "rs1065712_C\n",
      "rs73223431_T\n",
      "rs11787077_T\n",
      "rs10933431_G\n",
      "rs7068231_T\n",
      "rs6586028_C\n",
      "rs6846529_C\n",
      "rs10437655_A\n",
      "rs1582763_A\n",
      "rs3851179_T\n",
      "rs74685827_G\n",
      "rs11218343_C\n",
      "rs6489896_C\n",
      "rs17125924_G\n",
      "rs6943429_T\n",
      "rs149080927_G\n",
      "rs9304690_T\n",
      "rs587709_C\n",
      "rs6014724_G\n",
      "rs6742_T\n",
      "rs2154481_C\n",
      "rs2830489_T\n",
      "rs13276936_T\n",
      "rs138799625_T\n",
      "rs112131072_A\n",
      "rs11771145_A\n",
      "rs1065712_C\n",
      "rs73223431_T\n",
      "rs1532276_T\n",
      "rs79832570_T\n",
      "rs7912495_A\n",
      "rs7068231_T\n",
      "rs1902660_C\n",
      "rs4434960_A\n",
      "rs7232_A\n",
      "rs10792832_A\n",
      "rs117807585_A\n",
      "rs74825460_T\n",
      "rs12590654_A\n",
      "rs602602_A\n",
      "rs117618017_T\n",
      "rs1140239_T\n",
      "rs679515_T\n",
      "rs6733839_T\n",
      "rs10933431_C\n",
      "rs6827852_T\n",
      "rs2245466_C\n",
      "rs62374257_T\n",
      "rs113706587_A\n",
      "rs6605556_A\n",
      "rs9394766_A\n",
      "rs7754282_C\n",
      "rs74504435_A\n",
      "rs7384878_T\n",
      "rs78924645_A\n",
      "rs4407053_A\n",
      "rs56407236_A\n",
      "rs57402520_A\n",
      "rs5848_T\n",
      "rs199498_T\n",
      "rs4311_T\n",
      "rs12151021_A\n",
      "rs5167_T\n",
      "rs12984029_A\n",
      "rs6014724_A\n",
      "rs2154481_T\n",
      "rs112437613_T\n",
      "rs429358_C\n",
      "rs7412_C\n",
      "rs7412_C\n",
      "rs111371860_A\n",
      "rs440446_G\n",
      "rs449647_A\n",
      "rs35568738_G\n",
      "rs71352239_C\n",
      "rs2967668_A\n",
      "rs76692773_C\n",
      "rs640345_T\n",
      "rs4803748_C\n",
      "rs13025717_T\n",
      "rs3818361_A\n",
      "rs35336243_T\n",
      "rs7561528_A\n",
      "rs79638902_T\n",
      "rs74846209_T\n",
      "rs6859_A\n",
      "rs4803764_C\n",
      "rs192775394_C\n",
      "rs60049679_C\n",
      "rs144261139_A\n",
      "rs141441332_A\n",
      "rs28399664_G\n",
      "rs138607350_G\n",
      "rs79701229_A\n",
      "rs429358_C\n",
      "rs6859_A\n",
      "rs7259620_G\n",
      "rs1160985_C\n",
      "rs405509_T\n",
      "rs769450_G\n",
      "rs8106922_A\n",
      "rs6859_A\n",
      "rs7259620_G\n",
      "rs769450_G\n",
      "rs6859_A\n",
      "rs405509_T\n",
      "rs8106922_A\n",
      "rs1160985_C\n",
      "rs7259620_G\n",
      "rs7412_C\n",
      "rs390082_G\n",
      "rs1338764_C\n",
      "rs116668122_T\n",
      "rs34710518_A\n",
      "rs972600_G\n",
      "rs6978419_A\n",
      "rs372758_A\n",
      "rs12256016_G\n",
      "rs72838408_A\n",
      "rs12785292_A\n",
      "rs896559_C\n",
      "rs11246969_A\n",
      "rs7155666_A\n",
      "rs429358_C\n",
      "rs191177039_A\n",
      "rs574718000_C\n",
      "rs186375628_T\n",
      "rs188208995_G\n",
      "rs546144487_A\n",
      "rs568436451_T\n",
      "rs429358_T\n",
      "rs72824905_G\n",
      "rs56407236_A\n",
      "rs7225151_A\n",
      "rs5848_T\n",
      "rs199515_G\n",
      "rs4277405_C\n",
      "rs616338_T\n",
      "rs12151021_A\n",
      "rs149080927_G\n",
      "rs6014724_G\n",
      "rs2154481_C\n",
      "rs2830489_T\n",
      "rs6846529_C\n",
      "rs62374257_C\n",
      "rs113706587_A\n",
      "rs6605556_G\n",
      "rs143332484_T\n",
      "rs75932628_T\n",
      "rs7767350_T\n",
      "rs7384878_C\n",
      "rs11771145_A\n",
      "rs1065712_C\n",
      "rs73223431_T\n",
      "rs11787077_T\n",
      "rs34173062_A\n",
      "rs7912495_G\n",
      "rs7068231_T\n",
      "rs6586028_C\n",
      "rs10437655_A\n",
      "rs1582763_A\n",
      "rs3851179_T\n",
      "rs74685827_G\n",
      "rs11218343_C\n",
      "rs17125924_G\n",
      "rs12590654_A\n",
      "rs602602_A\n",
      "rs117618017_T\n",
      "rs1140239_T\n",
      "rs889555_T\n",
      "rs12446759_G\n",
      "rs679515_T\n",
      "rs143080277_C\n",
      "rs6733839_T\n",
      "rs10933431_G\n",
      "rs3822030_G\n",
      "rs429358_T\n",
      "rs429358_T\n",
      "rs429358_T\n",
      "rs188420713_T\n",
      "rs4557697_A\n",
      "rs147525344_T\n",
      "rs147991290_T\n",
      "rs4420638_A\n",
      "rs17639982_C\n",
      "rs183826639_T\n",
      "rs75885813_A\n",
      "rs56131196_A\n",
      "rs12041233_A\n",
      "rs115291397_A\n",
      "rs150511909_T\n",
      "rs118099348_T\n",
      "rs145049847_C\n",
      "rs187293782_T\n",
      "rs117483990_T\n",
      "rs118152978_A\n",
      "rs61860854_T\n",
      "rs115514216_T\n",
      "rs139675748_T\n",
      "rs147028191_A\n",
      "rs183757472_A\n",
      "rs143638193_T\n",
      "rs76930906_A\n",
      "rs9381563_C\n",
      "rs2081545_A\n",
      "rs4236673_A\n",
      "rs4663105_C\n",
      "rs7810606_T\n",
      "rs7412_T\n",
      "rs138607350_G\n",
      "rs79701229_A\n",
      "rs10407439_A\n",
      "rs2965169_C\n",
      "rs116949436_A\n",
      "rs55923289_C\n",
      "rs28469095_C\n",
      "rs111371860_T\n",
      "rs2967668_G\n",
      "rs76692773_T\n",
      "rs11672748_G\n",
      "rs595290_C\n",
      "rs346757_C\n",
      "rs34874378_A\n",
      "rs183427010_A\n",
      "rs111740474_A\n",
      "rs9331896_C\n",
      "rs983392_G\n",
      "rs114360492_T\n",
      "rs187370608_A\n",
      "rs41289512_G\n",
      "rs6014724_G\n",
      "rs113260531_A\n",
      "rs6931277_T\n",
      "rs111278892_G\n",
      "rs2093760_A\n",
      "rs10933431_G\n",
      "rs4575098_A\n",
      "rs6448453_A\n",
      "rs1859788_A\n",
      "rs867611_G\n",
      "rs3865444_A\n",
      "rs442495_C\n",
      "rs12590654_A\n",
      "rs1160984_T\n",
      "rs11665676_T\n",
      "rs561654715_A\n",
      "rs55840414_A\n",
      "rs112481437_A\n",
      "rs203709_A\n",
      "rs11881756_C\n",
      "rs76856627_G\n",
      "rs60239918_T\n",
      "rs41290100_T\n",
      "rs846876_C\n",
      "rs147188206_C\n",
      "rs2965164_T\n",
      "rs41290108_C\n",
      "rs547509922_T\n",
      "rs62118504_G\n",
      "rs35194383_T\n",
      "rs80257887_A\n",
      "rs75161053_A\n",
      "rs142412517_T\n",
      "rs12461144_T\n",
      "rs142092405_G\n",
      "rs150820726_T\n",
      "rs183321458_A\n",
      "rs539159088_C\n",
      "rs576651896_G\n",
      "rs186110295_T\n",
      "rs78986976_A\n",
      "rs204907_G\n",
      "rs9469112_T\n",
      "rs141864196_A\n",
      "rs114036675_A\n",
      "rs3752241_G\n",
      "rs11083743_C\n",
      "rs34181358_A\n",
      "rs204480_T\n",
      "rs10421247_C\n",
      "rs419010_C\n",
      "rs10792832_A\n",
      "rs1081105_C\n",
      "rs157580_G\n",
      "rs6859_A\n",
      "rs28399637_A\n",
      "rs11218343_C\n",
      "rs1476679_C\n",
      "rs35349669_T\n",
      "rs6656401_A\n",
      "rs6733839_T\n",
      "rs28834970_C\n",
      "rs429358_C\n",
      "rs9877502_A\n",
      "rs769449_A\n",
      "rs769449_A\n",
      "rs514716_G\n",
      "rs56131196_A\n",
      "rs6857_T\n",
      "rs59007384_T\n",
      "rs429358_C\n",
      "rs1936246_T\n",
      "rs442495_T\n",
      "rs4308_G\n",
      "rs4308_G\n",
      "rs4575098_A\n",
      "rs12969294_A\n",
      "rs79638902_T\n",
      "rs28399637_A\n",
      "rs116949436_A\n",
      "rs113321260_A\n",
      "rs11130222_A\n",
      "rs7599488_T\n",
      "rs12987662_A\n",
      "rs4663105_A\n",
      "rs17824247_T\n",
      "rs16845580_T\n",
      "rs3172494_T\n",
      "rs73082337_C\n",
      "rs34759087_T\n",
      "rs148734725_A\n",
      "rs7630869_T\n",
      "rs7614725_A\n",
      "rs3811695_T\n",
      "rs1138536_T\n",
      "rs6839705_A\n",
      "rs4863692_T\n",
      "rs61160187_A\n",
      "rs6449512_A\n",
      "rs10939881_A\n",
      "rs113474297_T\n",
      "rs10223052_A\n",
      "rs6882046_A\n",
      "rs3850651_T\n",
      "rs12514965_T\n",
      "rs7757476_A\n",
      "rs138002663_T\n",
      "rs77212406_T\n",
      "rs9401593_A\n",
      "rs10808026_A\n",
      "rs7982_A\n",
      "rs13266268_T\n",
      "rs3824874_T\n",
      "rs11218343_T\n",
      "rs2819336_T\n",
      "rs12410444_A\n",
      "rs34305371_A\n",
      "rs1008078_T\n",
      "rs11588857_A\n",
      "rs1752684_A\n",
      "rs35771425_T\n",
      "rs71537331_T\n",
      "rs11222416_T\n",
      "rs7964899_A\n",
      "rs2456973_A\n",
      "rs55742290_T\n",
      "rs1727302_A\n",
      "rs1969355_A\n",
      "rs6576_T\n",
      "rs9527702_A\n",
      "rs9556958_T\n",
      "rs34344888_A\n",
      "rs12590654_A\n",
      "rs1378214_T\n",
      "rs12900061_A\n",
      "rs28420834_A\n",
      "rs113568679_A\n",
      "rs9792504_A\n",
      "rs7029201_A\n",
      "rs2305192_T\n",
      "rs11191193_A\n",
      "rs10786662_C\n",
      "rs55970842_A\n",
      "rs3890065_C\n",
      "rs983392_A\n",
      "rs72924659_T\n",
      "rs10792832_A\n",
      "rs4663105_C\n",
      "rs429358_C\n",
      "rs3774745_T\n",
      "rs2584662_C\n",
      "rs117501883_A\n",
      "rs9270599_G\n",
      "rs4968782_G\n",
      "rs61812598_G\n",
      "rs6808835_G\n",
      "rs573521_A\n",
      "rs2228467_G\n",
      "rs2858331_G\n",
      "rs2584662_C\n",
      "rs12058296_A\n",
      "rs28895026_C\n",
      "rs4147104_A\n",
      "rs6695557_A\n",
      "rs429358_T\n",
      "rs73505251_A\n",
      "rs7738720_T\n",
      "rs73427293_A\n",
      "rs570487962_A\n",
      "rs429358_C\n",
      "rs1859788_A\n",
      "rs7982_A\n",
      "rs592297_C\n",
      "rs12453_C\n",
      "rs11554586_A\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = '../MONDO_0004975_associations_export.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Step 2: Check column names to confirm correct p-value column\n",
    "print(df.columns)\n",
    "\n",
    "# Step 3: Filter SNPs with p-value < 1e-5\n",
    "filtered_df = df[df['pValue'] < 1e-8]\n",
    "\n",
    "# Step 4: Keep only variantId ending with -A, -T, -C, or -G (exclude -?)\n",
    "filtered_df = filtered_df[filtered_df['riskAllele'].str.match(r'^rs\\d+-[ATCG]$', na=False)]\n",
    "#convert - to _\n",
    "filtered_df['riskAllele'] = filtered_df['riskAllele'].str.replace('-', '_')\n",
    "print(filtered_df['riskAllele'].head())\n",
    "p_value5_id = filtered_df['riskAllele'].tolist()\n",
    "# Step 5: Print the first 5 SNPs with p-value < 1e-5\n",
    "print(\"SNPs with p-value < 1e-5:\")\n",
    "for snp in p_value5_id:\n",
    "    print(snp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0306833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4557b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4144/738370358.py:1: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data_with_no_filtering = pd.read_csv('../adni_full.raw', delim_whitespace=True, usecols= lambda x: not x.endswith('.'), nrows=5)\n"
     ]
    }
   ],
   "source": [
    "data_with_no_filtering = pd.read_csv('../adni_full.raw', delim_whitespace=True, usecols= lambda x: not x.endswith('.'), nrows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cf6d3175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592538"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_with_no_filtering.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce1854be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rs6882046_A is in the list\n",
      "rs2718058_A is in the list\n",
      "rs11771145_G is in the list\n",
      "rs10498633_G is in the list\n",
      "rs8106922_A is in the list\n",
      "rs5167_T is in the list\n",
      "['rs6882046_A', 'rs2718058_A', 'rs11771145_G', 'rs10498633_G', 'rs8106922_A', 'rs5167_T']\n"
     ]
    }
   ],
   "source": [
    "id_of_pvalue5 = []\n",
    "for col in data_with_no_filtering.columns:\n",
    "    if col in p_value5_id:\n",
    "        print(col, \"is in the list\")\n",
    "        id_of_pvalue5.append(col)\n",
    "print(id_of_pvalue5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8bd75278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4144/990478919.py:8: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data_with_filtering = pd.read_csv('../adni_full.raw', delim_whitespace=True, usecols=columns_to_use)\n"
     ]
    }
   ],
   "source": [
    "# read the data again with the selected columns and first 6 columns of data and first 5 rows\n",
    "metadata_cols = ['IID', 'SEX', 'PHENOTYPE',]\n",
    "\n",
    "# Combine metadata columns with your filtered SNP rsIDs\n",
    "columns_to_use = metadata_cols + id_of_pvalue5\n",
    "\n",
    "# Read only those columns (first 5 + selected SNPs) and first 5 rows\n",
    "data_with_filtering = pd.read_csv('../adni_full.raw', delim_whitespace=True, usecols=columns_to_use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9cd9667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "IID             0\n",
      "SEX             0\n",
      "PHENOTYPE       0\n",
      "rs6882046_A     2\n",
      "rs2718058_A     1\n",
      "rs11771145_G    0\n",
      "rs10498633_G    0\n",
      "rs8106922_A     0\n",
      "rs5167_T        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#caclulate missing values of every column\n",
    "missing_values = data_with_filtering.isnull().sum()\n",
    "# Print the missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)\n",
    "#see the minimum and median and maximum of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3a92b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ADNIMERGE\n",
    "# adni_merge = pd.read_csv('../ADNIMERGE_11May2025.csv')\n",
    "#mergre adni merge with data_with_filtering on IID and get just dx\n",
    "# # Keep only relevant columns from ADNIMERGE\n",
    "# adni_merge = adni_merge[['RID', 'VISCODE', 'DX']]   \n",
    "# # Keep only baseline DX information\n",
    "# adni_dx = adni_merge[adni_merge['VISCODE'] == 'bl'][['RID', 'DX']]\n",
    "\n",
    "# # Extract RID (subject ID) from IID (e.g., '014_S_0520' â†’ 520)\n",
    "# data_with_filtering['RID'] = data_with_filtering['IID'].str.extract(r'_(\\d+)$').astype(float).astype(int)\n",
    "\n",
    "# # Merge DX info\n",
    "# data_with_filtering = pd.merge(data_with_filtering, adni_dx, on='RID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63a86a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing DX labels: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4144/4016535944.py:2: DtypeWarning: Columns (19,20,21,50,51,104,105,106) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  adni_merge = pd.read_csv('../ADNIMERGE_11May2025.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load ADNIMERGE\n",
    "adni_merge = pd.read_csv('../ADNIMERGE_11May2025.csv')\n",
    "\n",
    "# Filter to baseline diagnoses\n",
    "adni_dx = adni_merge[adni_merge['VISCODE'] == 'bl'][['RID', 'DX']]\n",
    "\n",
    "# Extract RID from IID if needed (only if IID is like '014_S_0520')\n",
    "data_with_filtering['RID'] = data_with_filtering['IID'].str.extract(r'_(\\d+)$')[0].astype(int)\n",
    "\n",
    "# Merge DX into your dataset\n",
    "data_with_filtering = data_with_filtering.merge(adni_dx, on='RID', how='left')\n",
    "\n",
    "# Drop RID if no longer needed\n",
    "data_with_filtering.drop(columns=['RID'], inplace=True)\n",
    "\n",
    "# Check merge result\n",
    "print(\"Missing DX labels:\", data_with_filtering['DX'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "49b5673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['IID', 'SEX', 'PHENOTYPE', 'rs6882046_A', 'rs2718058_A', 'rs11771145_G',\n",
      "       'rs10498633_G', 'rs8106922_A', 'rs5167_T', 'DX'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data_with_filtering.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f69fd3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [ 'IID','SEX','PHENOTYPE']  # Adjust the list based on your data\n",
    "\n",
    "# Drop the specified columns\n",
    "data_cleaned_with_just_dx_and_snp = data_with_filtering.drop(columns=columns_to_remove)\n",
    "#convert data_cleaned_with_just_dx_and_snp to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5861e592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4144/1545016660.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_cleaned_with_just_dx_and_snp[column].fillna(mode_value, inplace=True)\n",
      "/tmp/ipykernel_4144/1545016660.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_cleaned_with_just_dx_and_snp[column].fillna(mode_value, inplace=True)\n",
      "/tmp/ipykernel_4144/1545016660.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_cleaned_with_just_dx_and_snp[column].fillna(mode_value, inplace=True)\n",
      "/tmp/ipykernel_4144/1545016660.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_cleaned_with_just_dx_and_snp[column].fillna(mode_value, inplace=True)\n",
      "/tmp/ipykernel_4144/1545016660.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_cleaned_with_just_dx_and_snp[column].fillna(mode_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with mode for all columns except 'DX'\n",
    "for column in data_cleaned_with_just_dx_and_snp.columns:\n",
    "    if column != 'DX':\n",
    "        mode_value = data_cleaned_with_just_dx_and_snp[column].mode()[0]\n",
    "        data_cleaned_with_just_dx_and_snp[column].fillna(mode_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4875b526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4144/2107091537.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data_cleaned_with_just_dx_and_snp['DX'] = data_cleaned_with_just_dx_and_snp['DX'].replace(dx_mapping)\n"
     ]
    }
   ],
   "source": [
    "# Map DX values to numerical values\n",
    "dx_mapping = {'CN': 0, 'MCI': 0, 'Dementia': 1}\n",
    "data_cleaned_with_just_dx_and_snp['DX'] = data_cleaned_with_just_dx_and_snp['DX'].replace(dx_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "55a4f44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DX\n",
       "0    577\n",
       "1    180\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned_with_just_dx_and_snp['DX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80692057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rs6882046_A     float64\n",
       "rs2718058_A     float64\n",
       "rs11771145_G      int64\n",
       "rs10498633_G      int64\n",
       "rs8106922_A       int64\n",
       "rs5167_T          int64\n",
       "DX                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned_with_just_dx_and_snp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6fb42728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all float64 columns to int64\n",
    "for column in data_cleaned_with_just_dx_and_snp.columns:\n",
    "    if data_cleaned_with_just_dx_and_snp[column].dtype == 'float64':\n",
    "        data_cleaned_with_just_dx_and_snp[column] = data_cleaned_with_just_dx_and_snp[column].astype('int64')\n",
    "#save the data_cleaned_with_just_dx_and_snp to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "75dd0ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rs6882046_A     int64\n",
       "rs2718058_A     int64\n",
       "rs11771145_G    int64\n",
       "rs10498633_G    int64\n",
       "rs8106922_A     int64\n",
       "rs5167_T        int64\n",
       "DX              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned_with_just_dx_and_snp.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "af350c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#balance data of two classes\n",
    "# from sklearn.utils import resample\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# majority_class = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 0]\n",
    "# minority_class = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 1]\n",
    "\n",
    "# # Downsample majority class\n",
    "# majority_downsampled = resample(majority_class, \n",
    "#                                  replace=False,    # sample without replacement\n",
    "#                                  n_samples=len(minority_class),  # match minority class size\n",
    "#                                  random_state=42)\n",
    "\n",
    "# # Combine minority class with downsampled majority class\n",
    "# balanced_data = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "# # Shuffle the data\n",
    "# balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f58ecfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_data['DX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "97a8e54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rs6882046_A",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rs2718058_A",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rs11771145_G",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rs10498633_G",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rs8106922_A",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rs5167_T",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DX",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b1922067-2d4d-4e3a-a139-2aaa70796ffe",
       "rows": [
        [
         "count",
         "757.0",
         "757.0",
         "757.0",
         "757.0",
         "757.0",
         "757.0",
         "757.0"
        ],
        [
         "mean",
         "1.4108322324966975",
         "1.2509907529722588",
         "1.3289299867899604",
         "1.6142668428005285",
         "1.3513870541611626",
         "1.2113606340819023",
         "0.23778071334214002"
        ],
        [
         "std",
         "0.6477792504137264",
         "0.6960886157850068",
         "0.6657696676649303",
         "0.562691376142722",
         "0.6551899474096635",
         "0.6727719435477313",
         "0.4260056132700422"
        ],
        [
         "min",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "25%",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0"
        ],
        [
         "50%",
         "1.0",
         "1.0",
         "1.0",
         "2.0",
         "1.0",
         "1.0",
         "0.0"
        ],
        [
         "75%",
         "2.0",
         "2.0",
         "2.0",
         "2.0",
         "2.0",
         "2.0",
         "0.0"
        ],
        [
         "max",
         "2.0",
         "2.0",
         "2.0",
         "2.0",
         "2.0",
         "2.0",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rs6882046_A</th>\n",
       "      <th>rs2718058_A</th>\n",
       "      <th>rs11771145_G</th>\n",
       "      <th>rs10498633_G</th>\n",
       "      <th>rs8106922_A</th>\n",
       "      <th>rs5167_T</th>\n",
       "      <th>DX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>757.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>757.00000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>757.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.410832</td>\n",
       "      <td>1.250991</td>\n",
       "      <td>1.32893</td>\n",
       "      <td>1.614267</td>\n",
       "      <td>1.351387</td>\n",
       "      <td>1.211361</td>\n",
       "      <td>0.237781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.647779</td>\n",
       "      <td>0.696089</td>\n",
       "      <td>0.66577</td>\n",
       "      <td>0.562691</td>\n",
       "      <td>0.655190</td>\n",
       "      <td>0.672772</td>\n",
       "      <td>0.426006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rs6882046_A  rs2718058_A  rs11771145_G  rs10498633_G  rs8106922_A  \\\n",
       "count   757.000000   757.000000     757.00000    757.000000   757.000000   \n",
       "mean      1.410832     1.250991       1.32893      1.614267     1.351387   \n",
       "std       0.647779     0.696089       0.66577      0.562691     0.655190   \n",
       "min       0.000000     0.000000       0.00000      0.000000     0.000000   \n",
       "25%       1.000000     1.000000       1.00000      1.000000     1.000000   \n",
       "50%       1.000000     1.000000       1.00000      2.000000     1.000000   \n",
       "75%       2.000000     2.000000       2.00000      2.000000     2.000000   \n",
       "max       2.000000     2.000000       2.00000      2.000000     2.000000   \n",
       "\n",
       "         rs5167_T          DX  \n",
       "count  757.000000  757.000000  \n",
       "mean     1.211361    0.237781  \n",
       "std      0.672772    0.426006  \n",
       "min      0.000000    0.000000  \n",
       "25%      1.000000    0.000000  \n",
       "50%      1.000000    0.000000  \n",
       "75%      2.000000    0.000000  \n",
       "max      2.000000    1.000000  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get statistics of the data\n",
    "data_cleaned_with_just_dx_and_snp.describe()\n",
    "# balanced_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b5e9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy if always predict class 0: 76.22 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline accuracy if always predict class 0:\", round((577 / (577 + 180)) * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c07ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Logistic Regression Accuracy: 0.5627705627705628\n",
      "SMOTE Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.58      0.57       115\n",
      "           1       0.57      0.54      0.56       116\n",
      "\n",
      "    accuracy                           0.56       231\n",
      "   macro avg       0.56      0.56      0.56       231\n",
      "weighted avg       0.56      0.56      0.56       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Separate features and target\n",
    "X = data_cleaned_with_just_dx_and_snp.drop(columns=['DX'])\n",
    "y = data_cleaned_with_just_dx_and_snp['DX']\n",
    "\n",
    "# Apply SMOTE to balance the classes (AD = 1)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the balanced data\n",
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model (e.g., Logistic Regression) on SMOTE data\n",
    "logreg_smote = LogisticRegression(penalty='l2', random_state=42, solver='liblinear')\n",
    "logreg_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = logreg_smote.predict(X_test_smote)\n",
    "\n",
    "# Evaluate\n",
    "print(\"SMOTE Logistic Regression Accuracy:\", accuracy_score(y_test_smote, y_pred_smote))\n",
    "print(\"SMOTE Logistic Regression Classification Report:\\n\", classification_report(y_test_smote, y_pred_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f94c89de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampled Logistic Regression Accuracy: 0.4583333333333333\n",
      "Undersampled Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.41      0.47        41\n",
      "           1       0.40      0.52      0.45        31\n",
      "\n",
      "    accuracy                           0.46        72\n",
      "   macro avg       0.47      0.47      0.46        72\n",
      "weighted avg       0.47      0.46      0.46        72\n",
      "\n",
      "Undersampled SVM Accuracy: 0.4444444444444444\n",
      "Undersampled SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.34      0.41        41\n",
      "           1       0.40      0.58      0.47        31\n",
      "\n",
      "    accuracy                           0.44        72\n",
      "   macro avg       0.46      0.46      0.44        72\n",
      "weighted avg       0.47      0.44      0.44        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Separate AD (DX==1) and CN+MCI (DX==0)\n",
    "ad = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 1]\n",
    "cn_mci = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 0]\n",
    "\n",
    "# Downsample CN+MCI to match AD\n",
    "cn_mci_downsampled = resample(\n",
    "    cn_mci,\n",
    "    replace=False,\n",
    "    n_samples=len(ad),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine to create a balanced dataset\n",
    "undersampled_data = pd.concat([ad, cn_mci_downsampled])\n",
    "undersampled_data = undersampled_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split features and target\n",
    "X_under = undersampled_data.drop(columns=['DX'])\n",
    "y_under = undersampled_data['DX']\n",
    "\n",
    "# Train/test split\n",
    "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(\n",
    "    X_under, y_under, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_under = LogisticRegression(penalty='l2', solver='liblinear', random_state=42)\n",
    "logreg_under.fit(X_train_under, y_train_under)\n",
    "y_pred_logreg_under = logreg_under.predict(X_test_under)\n",
    "\n",
    "print(\"Undersampled Logistic Regression Accuracy:\", accuracy_score(y_test_under, y_pred_logreg_under))\n",
    "print(\"Undersampled Logistic Regression Classification Report:\\n\", classification_report(y_test_under, y_pred_logreg_under))\n",
    "\n",
    "# SVM\n",
    "svm_under = SVC(kernel='linear', random_state=42)\n",
    "svm_under.fit(X_train_under, y_train_under)\n",
    "y_pred_svm_under = svm_under.predict(X_test_under)\n",
    "\n",
    "print(\"Undersampled SVM Accuracy:\", accuracy_score(y_test_under, y_pred_svm_under))\n",
    "print(\"Undersampled SVM Classification Report:\\n\", classification_report(y_test_under, y_pred_svm_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2764b3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE+ENN Logistic Regression Accuracy: 0.6949152542372882\n",
      "SMOTE+ENN Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82        86\n",
      "           1       0.00      0.00      0.00        32\n",
      "\n",
      "    accuracy                           0.69       118\n",
      "   macro avg       0.36      0.48      0.41       118\n",
      "weighted avg       0.52      0.69      0.60       118\n",
      "\n",
      "SMOTE+ENN SVM ( linear ) Accuracy: 0.7288135593220338\n",
      "SMOTE+ENN SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84        86\n",
      "           1       0.00      0.00      0.00        32\n",
      "\n",
      "    accuracy                           0.73       118\n",
      "   macro avg       0.36      0.50      0.42       118\n",
      "weighted avg       0.53      0.73      0.61       118\n",
      "\n",
      "SMOTE+ENN SVM (RBF) Accuracy: 0.9576271186440678\n",
      "SMOTE+ENN SVM (RBF) Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97        86\n",
      "           1       0.89      0.97      0.93        32\n",
      "\n",
      "    accuracy                           0.96       118\n",
      "   macro avg       0.94      0.96      0.95       118\n",
      "weighted avg       0.96      0.96      0.96       118\n",
      "\n",
      "SMOTE+ENN SVM (Poly) Accuracy: 0.9576271186440678\n",
      "SMOTE+ENN SVM (Poly) Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97        86\n",
      "           1       0.91      0.94      0.92        32\n",
      "\n",
      "    accuracy                           0.96       118\n",
      "   macro avg       0.94      0.95      0.95       118\n",
      "weighted avg       0.96      0.96      0.96       118\n",
      "\n",
      "SMOTE+ENN Naive Bayes Accuracy: 0.7711864406779662\n",
      "SMOTE+ENN Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        86\n",
      "           1       0.67      0.31      0.43        32\n",
      "\n",
      "    accuracy                           0.77       118\n",
      "   macro avg       0.73      0.63      0.64       118\n",
      "weighted avg       0.75      0.77      0.74       118\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE+ENN MLP Accuracy: 0.9067796610169492\n",
      "SMOTE+ENN MLP Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94        86\n",
      "           1       0.84      0.81      0.83        32\n",
      "\n",
      "    accuracy                           0.91       118\n",
      "   macro avg       0.88      0.88      0.88       118\n",
      "weighted avg       0.91      0.91      0.91       118\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine SMOTE (oversampling) and ENN (undersampling)\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "\n",
    "# Split the combined balanced data\n",
    "X_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_comb = LogisticRegression(penalty='l2', solver='liblinear', random_state=42)\n",
    "logreg_comb.fit(X_train_comb, y_train_comb)\n",
    "y_pred_logreg_comb = logreg_comb.predict(X_test_comb)\n",
    "print(\"SMOTE+ENN Logistic Regression Accuracy:\", accuracy_score(y_test_comb, y_pred_logreg_comb))\n",
    "print(\"SMOTE+ENN Logistic Regression Classification Report:\\n\", classification_report(y_test_comb, y_pred_logreg_comb))\n",
    "\n",
    "# SVM\n",
    "svm_comb = SVC(kernel='linear', random_state=42)\n",
    "svm_comb.fit(X_train_comb, y_train_comb)\n",
    "y_pred_svm_comb = svm_comb.predict(X_test_comb)\n",
    "print(\"SMOTE+ENN SVM ( linear ) Accuracy:\", accuracy_score(y_test_comb, y_pred_svm_comb))\n",
    "print(\"SMOTE+ENN SVM Classification Report:\\n\", classification_report(y_test_comb, y_pred_svm_comb))\n",
    "# svm with rbf kernel\n",
    "svm_comb_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_comb_rbf.fit(X_train_comb, y_train_comb)\n",
    "y_pred_svm_comb_rbf = svm_comb_rbf.predict(X_test_comb)\n",
    "print(\"SMOTE+ENN SVM (RBF) Accuracy:\", accuracy_score(y_test_comb, y_pred_svm_comb_rbf))\n",
    "print(\"SMOTE+ENN SVM (RBF) Classification Report:\\n\", classification_report(y_test_comb, y_pred_svm_comb_rbf))  \n",
    "# svm with poly kernel\n",
    "svm_comb_poly = SVC(kernel='poly', random_state=42)\n",
    "svm_comb_poly.fit(X_train_comb, y_train_comb)\n",
    "y_pred_svm_comb_poly = svm_comb_poly.predict(X_test_comb)\n",
    "print(\"SMOTE+ENN SVM (Poly) Accuracy:\", accuracy_score(y_test_comb, y_pred_svm_comb_poly))\n",
    "print(\"SMOTE+ENN SVM (Poly) Classification Report:\\n\", classification_report(y_test_comb, y_pred_svm_comb_poly))\n",
    "# Naive Bayes\n",
    "nb_comb = GaussianNB()\n",
    "nb_comb.fit(X_train_comb, y_train_comb)\n",
    "y_pred_nb_comb = nb_comb.predict(X_test_comb)\n",
    "print(\"SMOTE+ENN Naive Bayes Accuracy:\", accuracy_score(y_test_comb, y_pred_nb_comb))\n",
    "print(\"SMOTE+ENN Naive Bayes Classification Report:\\n\", classification_report(y_test_comb, y_pred_nb_comb))\n",
    "# check mlp model with this balanced data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_comb = MLPClassifier(random_state=42)\n",
    "mlp_comb.fit(X_train_comb, y_train_comb)\n",
    "y_pred_mlp_comb = mlp_comb.predict(X_test_comb)\n",
    "print(\"SMOTE+ENN MLP Accuracy:\", accuracy_score(y_test_comb, y_pred_mlp_comb))\n",
    "print(\"SMOTE+ENN MLP Classification Report:\\n\", classification_report(y_test_comb, y_pred_mlp_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8470389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training set: DX\n",
      "0    228\n",
      "1     86\n",
      "Name: count, dtype: int64\n",
      "Class distribution in test set: DX\n",
      "0    57\n",
      "1    22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Logistic Regression ---\n",
      "Accuracy: 0.6835\n",
      "Balanced Accuracy: 0.7249\n",
      "F1 Macro Score: 0.6662\n",
      "Confusion Matrix:\n",
      " [[36 21]\n",
      " [ 4 18]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.63      0.74        57\n",
      "           1       0.46      0.82      0.59        22\n",
      "\n",
      "    accuracy                           0.68        79\n",
      "   macro avg       0.68      0.72      0.67        79\n",
      "weighted avg       0.78      0.68      0.70        79\n",
      "\n",
      "ROC AUC Score: 0.7576\n",
      "\n",
      "--- SVM (Linear) ---\n",
      "Accuracy: 0.6835\n",
      "Balanced Accuracy: 0.7528\n",
      "F1 Macro Score: 0.6733\n",
      "Confusion Matrix:\n",
      " [[34 23]\n",
      " [ 2 20]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.60      0.73        57\n",
      "           1       0.47      0.91      0.62        22\n",
      "\n",
      "    accuracy                           0.68        79\n",
      "   macro avg       0.70      0.75      0.67        79\n",
      "weighted avg       0.81      0.68      0.70        79\n",
      "\n",
      "ROC AUC Score: 0.756\n",
      "\n",
      "--- SVM (RBF) ---\n",
      "Accuracy: 0.9367\n",
      "Balanced Accuracy: 0.9561\n",
      "F1 Macro Score: 0.926\n",
      "Confusion Matrix:\n",
      " [[52  5]\n",
      " [ 0 22]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        57\n",
      "           1       0.81      1.00      0.90        22\n",
      "\n",
      "    accuracy                           0.94        79\n",
      "   macro avg       0.91      0.96      0.93        79\n",
      "weighted avg       0.95      0.94      0.94        79\n",
      "\n",
      "ROC AUC Score: 0.9952\n",
      "\n",
      "--- SVM (Poly) ---\n",
      "Accuracy: 0.9367\n",
      "Balanced Accuracy: 0.9561\n",
      "F1 Macro Score: 0.926\n",
      "Confusion Matrix:\n",
      " [[52  5]\n",
      " [ 0 22]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        57\n",
      "           1       0.81      1.00      0.90        22\n",
      "\n",
      "    accuracy                           0.94        79\n",
      "   macro avg       0.91      0.96      0.93        79\n",
      "weighted avg       0.95      0.94      0.94        79\n",
      "\n",
      "ROC AUC Score: 0.9681\n",
      "\n",
      "--- Naive Bayes ---\n",
      "Accuracy: 0.8101\n",
      "Balanced Accuracy: 0.701\n",
      "F1 Macro Score: 0.7247\n",
      "Confusion Matrix:\n",
      " [[54  3]\n",
      " [12 10]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88        57\n",
      "           1       0.77      0.45      0.57        22\n",
      "\n",
      "    accuracy                           0.81        79\n",
      "   macro avg       0.79      0.70      0.72        79\n",
      "weighted avg       0.80      0.81      0.79        79\n",
      "\n",
      "ROC AUC Score: 0.8668\n",
      "\n",
      "--- MLP (Neural Net) ---\n",
      "Accuracy: 0.962\n",
      "Balanced Accuracy: 0.9737\n",
      "F1 Macro Score: 0.9546\n",
      "Confusion Matrix:\n",
      " [[54  3]\n",
      " [ 0 22]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        57\n",
      "           1       0.88      1.00      0.94        22\n",
      "\n",
      "    accuracy                           0.96        79\n",
      "   macro avg       0.94      0.97      0.95        79\n",
      "weighted avg       0.97      0.96      0.96        79\n",
      "\n",
      "ROC AUC Score: 0.9864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# === 1. Balance the dataset using SMOTE + ENN === #\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)  # X, y must be pre-defined\n",
    "\n",
    "# === 2. Train-test split === #\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# === 3. Feature scaling (IMPORTANT for SVM, MLP, Logistic Regression) === #\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# see data balance\n",
    "print(\"Class distribution in training set:\", pd.Series(y_train).value_counts())\n",
    "print(\"Class distribution in test set:\", pd.Series(y_test).value_counts())\n",
    "# === 4. Define and train models === #\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        penalty='l2', solver='liblinear', class_weight='balanced', random_state=42\n",
    "    ),\n",
    "    \"SVM (Linear)\": SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42),\n",
    "    \"SVM (RBF)\": SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42),\n",
    "    \"SVM (Poly)\": SVC(kernel='poly', class_weight='balanced', probability=True, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"MLP (Neural Net)\": MLPClassifier(random_state=42, max_iter=500)\n",
    "    \n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(\"Accuracy:\", round(acc, 4))\n",
    "    print(\"Balanced Accuracy:\", round(bal_acc, 4))\n",
    "    print(\"F1 Macro Score:\", round(f1, 4))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    # test with auroc\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(\"ROC AUC Score:\", round(roc_auc, 4))\n",
    "    else:\n",
    "        print(\"ROC AUC Score: Not available for this model (no predict_proba)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8aceb270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Accuracy: 0.75\n",
      "Linear SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86       114\n",
      "           1       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.75       152\n",
      "   macro avg       0.38      0.50      0.43       152\n",
      "weighted avg       0.56      0.75      0.64       152\n",
      "\n",
      "Linear SVM ROC AUC Score: 0.49746075715604804\n",
      "Polynomial SVM Accuracy: 0.75\n",
      "Polynomial SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86       114\n",
      "           1       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.75       152\n",
      "   macro avg       0.38      0.50      0.43       152\n",
      "weighted avg       0.56      0.75      0.64       152\n",
      "\n",
      "Polynomial SVM ROC AUC Score: 0.5832179132040628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF SVM Accuracy: 0.75\n",
      "RBF SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86       114\n",
      "           1       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.75       152\n",
      "   macro avg       0.38      0.50      0.43       152\n",
      "weighted avg       0.56      0.75      0.64       152\n",
      "\n",
      "RBF SVM ROC AUC Score: 0.5319713758079408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Separate features and target\n",
    "X = data_cleaned_with_just_dx_and_snp.drop(columns=['DX'])\n",
    "y = data_cleaned_with_just_dx_and_snp['DX']\n",
    "\n",
    "# Split the data (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SVM models with different kernels\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_poly = SVC(kernel='poly', random_state=42)\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Train and evaluate each model\n",
    "svm_models = {'Linear': svm_linear, 'Polynomial': svm_poly, 'RBF': svm_rbf}\n",
    "for name, model in svm_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name} SVM Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"{name} SVM Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "    # evaluate with ROC AUC\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    y_pred_proba = model.decision_function(X_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"{name} SVM ROC AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c4d16fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.6842105263157895\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.86      0.80       114\n",
      "           1       0.27      0.16      0.20        38\n",
      "\n",
      "    accuracy                           0.68       152\n",
      "   macro avg       0.51      0.51      0.50       152\n",
      "weighted avg       0.63      0.68      0.65       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87113a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [10:30:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.7039473684210527\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82       114\n",
      "           1       0.32      0.16      0.21        38\n",
      "\n",
      "    accuracy                           0.70       152\n",
      "   macro avg       0.54      0.52      0.51       152\n",
      "weighted avg       0.65      0.70      0.67       152\n",
      "\n",
      "Gradient Boosting Accuracy: 0.7236842105263158\n",
      "Gradient Boosting Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.95      0.84       114\n",
      "           1       0.25      0.05      0.09        38\n",
      "\n",
      "    accuracy                           0.72       152\n",
      "   macro avg       0.50      0.50      0.46       152\n",
      "weighted avg       0.62      0.72      0.65       152\n",
      "\n",
      "AdaBoost Accuracy: 0.75\n",
      "AdaBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86       114\n",
      "           1       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.75       152\n",
      "   macro avg       0.38      0.50      0.43       152\n",
      "weighted avg       0.56      0.75      0.64       152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"Gradient Boosting Classification Report:\\n\", classification_report(y_test, y_pred_gb))\n",
    "\n",
    "# AdaBoost\n",
    "ada_model = AdaBoostClassifier(random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_ada))\n",
    "print(\"AdaBoost Classification Report:\\n\", classification_report(y_test, y_pred_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ae5e34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# import pandas as pd\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Random Sub-Sampling\n",
    "# # Separate classes\n",
    "# class_0 = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 0]\n",
    "# class_1 = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 1]\n",
    "# class_2 = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 2]\n",
    "\n",
    "# # Downsample majority classes to match the minority class size\n",
    "# min_class_size = min(len(class_0), len(class_1), len(class_2))\n",
    "# class_0_downsampled = resample(class_0, replace=False, n_samples=min_class_size, random_state=42)\n",
    "# class_1_downsampled = resample(class_1, replace=False, n_samples=min_class_size, random_state=42)\n",
    "# class_2_downsampled = resample(class_2, replace=False, n_samples=min_class_size, random_state=42)\n",
    "\n",
    "# # Combine downsampled classes\n",
    "# balanced_data_subsampling = pd.concat([class_0_downsampled, class_1_downsampled, class_2_downsampled])\n",
    "# print(\"Random Sub-Sampling Balanced Data:\")\n",
    "# print(balanced_data_subsampling['DX'].value_counts())\n",
    "\n",
    "# # Oversampling using SMOTE\n",
    "# X = data_cleaned_with_just_dx_and_snp.drop(columns=['DX'])\n",
    "# y = data_cleaned_with_just_dx_and_snp['DX']\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_oversampled, y_oversampled = smote.fit_resample(X, y)\n",
    "\n",
    "# # Combine oversampled data into a DataFrame\n",
    "# balanced_data_oversampling = pd.concat([pd.DataFrame(X_oversampled, columns=X.columns), pd.Series(y_oversampled, name='DX')], axis=1)\n",
    "# print(\"\\nSMOTE Oversampling Balanced Data:\")\n",
    "# print(balanced_data_oversampling['DX'].value_counts())\n",
    "\n",
    "# # Class-Weighted Model (Example: SVM with class weights)\n",
    "# class_weights = {0: 1.0, 1: 1.5, 2: 2.0}  # Adjust weights based on class imbalance\n",
    "# svm_model = SVC(kernel='linear', class_weight=class_weights, random_state=42)\n",
    "# svm_model.fit(X, y)\n",
    "\n",
    "# print(\"\\nClass-Weighted SVM Model Trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b661c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Define a function to train and evaluate models\n",
    "# def evaluate_models(X_train, X_test, y_train, y_test):\n",
    "#     models = {\n",
    "#         \"SVM\": SVC(kernel='linear', random_state=42),\n",
    "#         \"Logistic Regression\": LogisticRegression(random_state=42, solver='liblinear'),\n",
    "#         \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "#     }\n",
    "    \n",
    "#     results = {}\n",
    "#     for name, model in models.items():\n",
    "#         # Train the model\n",
    "#         model.fit(X_train, y_train)\n",
    "        \n",
    "#         # Predict on the test set\n",
    "#         y_pred = model.predict(X_test)\n",
    "        \n",
    "#         # Evaluate the model\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "#         report = classification_report(y_test, y_pred)\n",
    "        \n",
    "#         results[name] = {\n",
    "#             \"Accuracy\": accuracy,\n",
    "#             \"Classification Report\": report\n",
    "#         }\n",
    "        \n",
    "#         print(f\"{name} Results:\")\n",
    "#         print(f\"Accuracy: {accuracy}\")\n",
    "#         print(\"Classification Report:\")\n",
    "#         print(report)\n",
    "#         print(\"-\" * 50)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Evaluate on subsampling balanced data\n",
    "# print(\"Evaluating on Subsampling Balanced Data:\")\n",
    "# X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(\n",
    "#     balanced_data_subsampling.drop(columns=['DX']),\n",
    "#     balanced_data_subsampling['DX'],\n",
    "#     test_size=0.2,\n",
    "#     random_state=42\n",
    "# )\n",
    "# results_subsampling = evaluate_models(X_train_sub, X_test_sub, y_train_sub, y_test_sub)\n",
    "\n",
    "# # Evaluate on SMOTE balanced data\n",
    "# print(\"\\nEvaluating on SMOTE Balanced Data:\")\n",
    "# X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(\n",
    "#     X_oversampled,\n",
    "#     y_oversampled,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42\n",
    "# )\n",
    "# results_smote = evaluate_models(X_train_smote, X_test_smote, y_train_smote, y_test_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5236f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Random Sub-Sampling\n",
    "# # Separate classes\n",
    "# class_0 = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 0]\n",
    "# class_1 = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 1]\n",
    "# class_2 = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == 2]\n",
    "\n",
    "# # Downsample majority classes to match the minority class size\n",
    "# min_class_size = min(len(class_0), len(class_1), len(class_2))\n",
    "# class_0_downsampled = resample(class_0, replace=False, n_samples=min_class_size, random_state=42)\n",
    "# class_1_downsampled = resample(class_1, replace=False, n_samples=min_class_size, random_state=42)\n",
    "# class_2_downsampled = resample(class_2, replace=False, n_samples=min_class_size, random_state=42)\n",
    "\n",
    "# # Combine downsampled classes\n",
    "# balanced_data_subsampling = pd.concat([class_0_downsampled, class_1_downsampled, class_2_downsampled])\n",
    "# balanced_data_subsampling[\"DX\"].value_counts()\n",
    "# # Oversampling using SMOTE\n",
    "# X = data_cleaned_with_just_dx_and_snp.drop(columns=['DX'])\n",
    "# y = data_cleaned_with_just_dx_and_snp['DX']\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_oversampled, y_oversampled = smote.fit_resample(X, y)\n",
    "\n",
    "# # Class-Weighted Model (SVM with class weights)\n",
    "# class_weights = {0: 1.0, 1: 1.5, 2: 2.0}  # Adjust weights based on class imbalance\n",
    "# svm_model = SVC(kernel='linear', class_weight=class_weights, random_state=42)\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the SVM model\n",
    "# y_pred_svm = svm_model.predict(X_test)\n",
    "# print(\"SVM with Class Weights - Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# # Class-Weighted Model (Random Forest with class weights)\n",
    "# rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the Random Forest model\n",
    "# y_pred_rf = rf_model.predict(X_test)\n",
    "# print(\"Random Forest with Class Weights - Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "07f70da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_data_subsampling[\"DX\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "07b39d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Initialize StratifiedKFold\n",
    "# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Store results for each fold\n",
    "# fold_accuracies = []\n",
    "# best_split = None\n",
    "# best_accuracy = 0\n",
    "\n",
    "# # Perform k-fold cross-validation\n",
    "# for train_index, test_index in kf.split(X, y):\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "#     # Train a model (e.g., Logistic Regression)\n",
    "#     model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     fold_accuracies.append(accuracy)\n",
    "    \n",
    "#     # Check if this is the best split\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_split = (train_index, test_index)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Fold Accuracies:\", fold_accuracies)\n",
    "# print(\"Best Accuracy:\", best_accuracy)\n",
    "# print(\"Best Split Indices:\", best_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "73c6e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # balance the 3 classes of dx\n",
    "# import numpy as np\n",
    "# import random\n",
    "# # Set a random seed for reproducibility\n",
    "# random.seed(42)\n",
    "# # Get the counts of each class\n",
    "# class_counts = data_cleaned_with_just_dx_and_snp['DX'].value_counts()\n",
    "# # Find the minimum class count\n",
    "# min_count = class_counts.min()\n",
    "# # Create a balanced DataFrame\n",
    "# balanced_data = pd.DataFrame()\n",
    "# for dx_class in class_counts.index:\n",
    "#     # Get the rows for the current class\n",
    "#     class_rows = data_cleaned_with_just_dx_and_snp[data_cleaned_with_just_dx_and_snp['DX'] == dx_class]\n",
    "#     # Randomly sample rows to match the minimum count\n",
    "#     sampled_rows = class_rows.sample(n=min_count, random_state=42)\n",
    "#     # Append the sampled rows to the balanced DataFrame\n",
    "#     balanced_data = pd.concat([balanced_data, sampled_rows], ignore_index=True)\n",
    "# # Shuffle the balanced DataFrame\n",
    "# balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "65ac44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_data['DX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "01861a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the correlation of the data\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Calculate the correlation matrix\n",
    "# correlation_matrix = balanced_data.corr()\n",
    "# # Set the size of the plot\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# # Create a heatmap\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "# plt.title('Correlation Heatmap')\n",
    "# plt.show()\n",
    "# # save the plot to a image file\n",
    "# plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ef1ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute mutual information\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# # Assume df_bal is your balanced DataFrame (540Ã—31), with 'dx' as target\n",
    "# X = balanced_data.drop(columns='DX')\n",
    "# y = balanced_data['DX']\n",
    "\n",
    "# # Compute mutual information for each SNP\n",
    "# mi_scores = mutual_info_classif(X, y, discrete_features=True, random_state=42)\n",
    "\n",
    "# # Create a DataFrame of scores\n",
    "# mi_df = pd.DataFrame({\n",
    "#     'SNP': X.columns,\n",
    "#     'MI Score': mi_scores\n",
    "# }).sort_values('MI Score', ascending=False)\n",
    "\n",
    "# print(\"Top SNPs by Mutual Information:\")\n",
    "# print(mi_df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7d21a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # 1. Select top-15 SNPs\n",
    "# top_snps = [\n",
    "#     'rs7155434_A','rs9832461_A','rs474951_T','rs17022021_T','rs8106922_A',\n",
    "#     'rs2970989_T','rs2718058_A','rs440277_G','rs5167_T','rs11771145_G',\n",
    "#     'rs29745_A','rs6882046_A','rs1923775_T','rs2456930_G','rs17785248_A'\n",
    "# ]\n",
    "\n",
    "# X = balanced_data[top_snps].copy()\n",
    "# y = balanced_data['DX'].copy()\n",
    "\n",
    "# # 2. Train/Test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, stratify=y, random_state=42\n",
    "# )\n",
    "\n",
    "# # 3. (Optional) SMOTE on training set\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# # 5. Train XGBoost\n",
    "# model = XGBClassifier(\n",
    "#     objective='multi:softprob',\n",
    "#     num_class=3,\n",
    "#     learning_rate=0.05,\n",
    "#     n_estimators=200,\n",
    "#     max_depth=4,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     eval_metric='mlogloss',\n",
    "#     use_label_encoder=False,\n",
    "#     random_state=42\n",
    "# )\n",
    "# model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# # 6. Evaluate\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# print(\"Macro F1-score:\", f1_score(y_test, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6001a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # X, y from your balanced df_bal\n",
    "# X = balanced_data.drop(columns='DX')\n",
    "# y = balanced_data['DX']\n",
    "\n",
    "# # The list of k-values to try\n",
    "# k_values = [10, 15, 20, 25, 30]\n",
    "\n",
    "# # Use StratifiedKFold for balanced splits\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# results = []\n",
    "# for k in k_values:\n",
    "#     pipe = Pipeline([\n",
    "#         ('select', SelectKBest(mutual_info_classif, k=k)),\n",
    "#         ('scale', StandardScaler()),            # optional for trees\n",
    "#         ('xgb', XGBClassifier(\n",
    "#             objective='multi:softprob',\n",
    "#             num_class=3,\n",
    "#             learning_rate=0.05,\n",
    "#             n_estimators=200,\n",
    "#             max_depth=4,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             eval_metric='mlogloss',\n",
    "#             random_state=42\n",
    "#         ))\n",
    "#     ])\n",
    "#     # Compute cross-val macro-F1\n",
    "#     scores = cross_val_score(pipe, X, y, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "#     results.append({'k': k, 'mean_f1': np.mean(scores), 'std_f1': np.std(scores)})\n",
    "#     print(f\"k={k:2d} â†’ macro-F1 = {np.mean(scores):.3f} Â± {np.std(scores):.3f}\")\n",
    "\n",
    "# # Turn into DataFrame for easy viewing\n",
    "# results_df = pd.DataFrame(results).sort_values('mean_f1', ascending=False)\n",
    "# print(\"\\nBest k by macro-F1:\\n\", results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5394a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Accuracy: 0.75\n",
      "Logistic Regression Accuracy: 0.75\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86       114\n",
      "           1       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.75       152\n",
      "   macro avg       0.38      0.50      0.43       152\n",
      "weighted avg       0.56      0.75      0.64       152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mohamed-megdoud/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_model = Lasso(alpha=0.01, random_state=42)  # Adjust alpha as needed\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using Lasso\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "y_pred_lasso_rounded = [round(pred) for pred in y_pred_lasso]  # Round predictions to nearest integer\n",
    "\n",
    "# Evaluate Lasso\n",
    "lasso_accuracy = accuracy_score(y_test, y_pred_lasso_rounded)\n",
    "print(f\"Lasso Regression Accuracy: {lasso_accuracy}\")\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(penalty='l2', solver='liblinear', random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using Logistic Regression\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "logistic_accuracy = accuracy_score(y_test, y_pred_logistic)\n",
    "print(f\"Logistic Regression Accuracy: {logistic_accuracy}\")\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4864f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Confusion Matrix:\")\n",
    "# for row in conf_matrix:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1cb6e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# # Ensure y_train_encoded and y_test_encoded are one-hot encoded\n",
    "# y_train_encoded_onehot = to_categorical(y_train_encoded, num_classes=3)\n",
    "# y_test_encoded_onehot = to_categorical(y_test_encoded, num_classes=3)\n",
    "\n",
    "# # Scale the input features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Define the deep learning model\n",
    "# def create_3_class_model(input_dim):\n",
    "#     model = Sequential([\n",
    "#         Dense(128, activation='relu', input_dim=input_dim),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(64, activation='relu'),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(32, activation='relu'),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(3, activation='softmax')  # 3-class classification\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Create the model\n",
    "# input_dim = X_train_scaled.shape[1]\n",
    "# model = create_3_class_model(input_dim)\n",
    "\n",
    "# # Define early stopping to prevent overfitting\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train_scaled, y_train_encoded_onehot,\n",
    "#     validation_data=(X_test_scaled, y_test_encoded_onehot),\n",
    "#     epochs=100,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_encoded_onehot, verbose=0)\n",
    "# print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred_dl = model.predict(X_test_scaled)\n",
    "# y_pred_classes = y_pred_dl.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e8b06dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, SimpleRNN\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Scale the input features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Reshape data for CNN and RNN\n",
    "# X_train_cnn_rnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "# X_test_cnn_rnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# # Define early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # MLP Model\n",
    "# def create_mlp_model(input_dim):\n",
    "#     model = Sequential([\n",
    "#         Dense(128, activation='relu', input_dim=input_dim),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(64, activation='relu'),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(1, activation='sigmoid')  # Binary classification\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # CNN Model\n",
    "# def create_cnn_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "#         MaxPooling1D(pool_size=2),\n",
    "#         Dropout(0.3),\n",
    "#         Conv1D(64, kernel_size=3, activation='relu'),\n",
    "#         MaxPooling1D(pool_size=2),\n",
    "#         Dropout(0.3),\n",
    "#         Flatten(),\n",
    "#         Dense(64, activation='relu'),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(1, activation='sigmoid')  # Binary classification\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # RNN Model\n",
    "# def create_rnn_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         SimpleRNN(64, activation='relu', input_shape=input_shape, return_sequences=True),\n",
    "#         Dropout(0.3),\n",
    "#         SimpleRNN(32, activation='relu'),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(1, activation='sigmoid')  # Binary classification\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Train and evaluate MLP\n",
    "# mlp_model = create_mlp_model(X_train_scaled.shape[1])\n",
    "# mlp_history = mlp_model.fit(\n",
    "#     X_train_scaled, y_train,\n",
    "#     validation_data=(X_test_scaled, y_test),\n",
    "#     epochs=100,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "# mlp_test_loss, mlp_test_accuracy = mlp_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "# print(f\"MLP Test Accuracy: {mlp_test_accuracy}\")\n",
    "\n",
    "# # Train and evaluate CNN\n",
    "# cnn_model = create_cnn_model((X_train_cnn_rnn.shape[1], 1))\n",
    "# cnn_history = cnn_model.fit(\n",
    "#     X_train_cnn_rnn, y_train,\n",
    "#     validation_data=(X_test_cnn_rnn, y_test),\n",
    "#     epochs=100,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "# cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(X_test_cnn_rnn, y_test, verbose=0)\n",
    "# print(f\"CNN Test Accuracy: {cnn_test_accuracy}\")\n",
    "\n",
    "# # Train and evaluate RNN\n",
    "# rnn_model = create_rnn_model((X_train_cnn_rnn.shape[1], 1))\n",
    "# rnn_history = rnn_model.fit(\n",
    "#     X_train_cnn_rnn, y_train,\n",
    "#     validation_data=(X_test_cnn_rnn, y_test),\n",
    "#     epochs=100,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "# rnn_test_loss, rnn_test_accuracy = rnn_model.evaluate(X_test_cnn_rnn, y_test, verbose=0)\n",
    "# print(f\"RNN Test Accuracy: {rnn_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "156e48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# # Align y_test and y_pred_regularized to have the same length\n",
    "# min_length = min(len(y_test), len(y_pred_regularized))\n",
    "# y_test_aligned = y_test[:min_length]\n",
    "# y_pred_regularized_aligned = y_pred_regularized[:min_length]\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test_aligned, y_pred_regularized_aligned)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # Generate classification report for per-class F1 scores\n",
    "# class_report = classification_report(y_test_aligned, y_pred_regularized_aligned, target_names=['Class 0', 'Class 1', 'Class 2'])\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5d0943c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Initialize the Random Forest classifier\n",
    "# rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Train the model on the training data\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test data\n",
    "# y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "# print(f\"Random Forest Accuracy: {rf_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "54f8c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Define SVM models with different kernels\n",
    "# svm_linear = SVC(kernel='linear', random_state=42)\n",
    "# svm_poly = SVC(kernel='poly', random_state=42)\n",
    "# svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# # Train and evaluate each model\n",
    "# svm_models = {'Linear': svm_linear, 'Polynomial': svm_poly, 'RBF': svm_rbf}\n",
    "# svm_accuracies = {}\n",
    "\n",
    "# for name, model in svm_models.items():\n",
    "#     # Train the model\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Predict on the test set\n",
    "#     y_pred = model.predict(X_test)\n",
    "    \n",
    "#     # Calculate accuracy\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     svm_accuracies[name] = accuracy\n",
    "#     print(f\"{name} SVM Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3955049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the custom deep learning model\n",
    "# def create_model(input_dim):\n",
    "#     model = Sequential([\n",
    "#         Dense(128, activation='relu', input_dim=input_dim),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(64, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(32, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(1, activation='sigmoid')  # Binary classification\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Prepare the data\n",
    "# # Fit the scaler on the training data and transform both training and test data\n",
    "# scaler.fit(X_train)  # Ensure the scaler is fitted on the current training data\n",
    "# X_train_scaled = scaler.transform(X_train)  # Scale the training data\n",
    "# X_test_scaled = scaler.transform(X_test)    # Scale the test data\n",
    "\n",
    "# # Create the model\n",
    "# input_dim = X_train_scaled.shape[1]\n",
    "# model = create_model(input_dim)\n",
    "\n",
    "# # Define early stopping to prevent overfitting\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train_scaled, y_train,\n",
    "#     validation_data=(X_test_scaled, y_test),\n",
    "#     epochs=100,\n",
    "#     batch_size=32,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "# print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred_dl = (model.predict(X_test_scaled) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "376db89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Define the parameter grid for RandomForestClassifier\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'max_depth': [10, 20, 30, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # Initialize the RandomForestClassifier\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# # Predict on the test data\n",
    "# y_pred_rf_optimized = best_rf_model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# optimized_rf_accuracy = accuracy_score(y_test, y_pred_rf_optimized)\n",
    "# print(f\"Optimized Random Forest Accuracy: {optimized_rf_accuracy}\")\n",
    "# print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "98ee9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Initialize Logistic Regression with L2 regularization and class_weight='balanced'\n",
    "# logreg_model = LogisticRegression( penalty='l2', solver='liblinear', random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model with auroc\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# logreg_auc = roc_auc_score(y_test, y_pred_logreg)\n",
    "# print(f\"Logistic Regression AUC: {logreg_auc}\")\n",
    "# # Calculate accuracy\n",
    "# logreg_accuracy = accuracy_score(y_test, y_pred_logreg)\n",
    "# print(f\"Logistic Regression Accuracy: {logreg_accuracy}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a5aa70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Define the Random Forest model with specified parameters\n",
    "# rf_model_2 = RandomForestClassifier(\n",
    "#     n_estimators=200,  # Example value within the range 100â€“300\n",
    "#     max_depth=7,       # Example value within the range 5â€“10\n",
    "#       # Add class_weight='balanced' if needed\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# rf_model_2.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred_rf_2 = rf_model_2.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# rf_accuracy_2 = accuracy_score(y_test, y_pred_rf_2)\n",
    "# print(f\"Random Forest Model 2 Accuracy: {rf_accuracy_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "54550608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid for RandomForestClassifier\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # Initialize the RandomForestClassifier\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(f\"Best Parameters: {best_params}\")\n",
    "# print(f\"Best Cross-Validation Accuracy: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "10843a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2]\n",
    "# }\n",
    "\n",
    "# # Initialize the XGBClassifier\n",
    "# xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(f\"Best Parameters: {best_params}\")\n",
    "# print(f\"Best Cross-Validation Accuracy: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9ae29b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid\n",
    "# params = {\n",
    "#     'learning_rate': [0.01],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'n_estimators': [100, 200, 300]\n",
    "# }\n",
    "\n",
    "# # Initialize the XGBClassifier\n",
    "# xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=xgb_model, param_grid=params, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Fit the model without early stopping in GridSearchCV\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Perform early stopping manually on the best model\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_model.set_params(early_stopping_rounds=10)\n",
    "# best_model.fit(\n",
    "#     X_train, y_train,\n",
    "#     eval_set=[(X_test, y_test)],\n",
    "#     verbose=False\n",
    "# )\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(f\"Best Parameters: {best_params}\")\n",
    "# print(f\"Best Cross-Validation Accuracy: {best_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
